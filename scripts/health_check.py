#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
health_check.py - Repository Health Checkä¸»æ£€æŸ¥è„šæœ¬

åŠŸèƒ½ï¼š
1. è¯»å–HEALTH_CHECK_MODEL.yamlè¯„åˆ†æ¨¡å‹
2. æ‰§è¡Œ5ä¸ªç»´åº¦çš„å¥åº·åº¦æ£€æŸ¥
3. è®¡ç®—åŠ æƒæ€»åˆ†ï¼ˆ100åˆ†åˆ¶ï¼‰
4. ç”Ÿæˆå¤šæ ¼å¼æŠ¥å‘Šï¼ˆconsole/markdown/json/htmlï¼‰
5. æä¾›æ™ºèƒ½æ¨èå»ºè®®

5ä¸ªç»´åº¦ï¼š
- Code Quality (25åˆ†): ä»£ç è´¨é‡ã€æµ‹è¯•è¦†ç›–ç‡ã€å¤æ‚åº¦ã€ç±»å‹å®‰å…¨
- Documentation (20åˆ†): æ¨¡å—æ–‡æ¡£è¦†ç›–ã€æ–‡æ¡£æ—¶æ•ˆæ€§ã€è´¨é‡ã€åŒæ­¥
- Architecture (20åˆ†): ä¾èµ–æ¸…æ™°åº¦ã€æ¨¡å—è€¦åˆåº¦ã€å¥‘çº¦ç¨³å®šæ€§ã€æ³¨å†Œè¡¨ä¸€è‡´æ€§
- AI Friendliness (20åˆ†): agent.mdè½»é‡åŒ–ã€æ–‡æ¡£èŒè´£åˆ†ç¦»ã€æ¨¡å—æ–‡æ¡£å®Œæ•´ã€å·¥ä½œæµå‹å¥½ã€è‡ªåŠ¨åŒ–è¦†ç›–
- Operations (15åˆ†): è¿ç§»å®Œæ•´æ€§ã€é…ç½®è§„èŒƒã€å¯è§‚æµ‹æ€§ã€å®‰å…¨å«ç”Ÿ

ç”¨æ³•ï¼š
    python scripts/health_check.py
    python scripts/health_check.py --format json
    python scripts/health_check.py --output report.md
    make health_check
    make health_report

Created: 2025-11-09 (Phase 14.2)
"""

import os
import sys
import json
import yaml
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from collections import defaultdict

# Windows UTF-8 support
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# è·¯å¾„è®¾ç½®
HERE = Path(__file__).parent.absolute()
REPO_ROOT = HERE.parent
MODEL_PATH = REPO_ROOT / "doc" / "process" / "HEALTH_CHECK_MODEL.yaml"
HISTORY_PATH = REPO_ROOT / "ai" / "maintenance_reports" / "health-history.json"


class HealthCheckEngine:
    """ä»“åº“å¥åº·åº¦æ£€æŸ¥å¼•æ“"""
    
    def __init__(self, model_path: Path = MODEL_PATH):
        """åˆå§‹åŒ–å¥åº·åº¦æ£€æŸ¥å¼•æ“"""
        self.model_path = model_path
        self.model = self._load_model()
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "dimensions": {},
            "total_score": 0,
            "grade": "",
            "recommendations": []
        }
    
    def _load_model(self) -> Dict[str, Any]:
        """åŠ è½½å¥åº·åº¦è¯„åˆ†æ¨¡å‹"""
        if not self.model_path.exists():
            print(f"âŒ å¥åº·åº¦æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}", file=sys.stderr)
            sys.exit(1)
        
        try:
            with open(self.model_path, 'r', encoding='utf-8') as f:
                model = yaml.safe_load(f)
            print(f"âœ“ å¥åº·åº¦æ¨¡å‹å·²åŠ è½½: {self.model_path.name}")
            return model
        except Exception as e:
            print(f"âŒ åŠ è½½å¥åº·åº¦æ¨¡å‹å¤±è´¥: {e}", file=sys.stderr)
            sys.exit(1)
    
    def check_code_quality(self) -> Dict[str, Any]:
        """æ£€æŸ¥ä»£ç è´¨é‡ç»´åº¦ï¼ˆ25åˆ†ï¼‰"""
        print("\nğŸ” æ£€æŸ¥ç»´åº¦ 1/5: Code Quality...")
        
        dimension = self.model["dimensions"]["code_quality"]
        metrics = dimension["metrics"]
        scores = {}
        
        # Metric 1.1: Linter Pass Rate (8 points)
        print("  - Linter Pass Rate...")
        linter_score = self._check_linter_pass_rate(metrics["linter_pass_rate"])
        scores["linter_pass_rate"] = linter_score
        
        # Metric 1.2: Test Coverage (7 points)
        print("  - Test Coverage...")
        coverage_score = self._check_test_coverage(metrics["test_coverage"])
        scores["test_coverage"] = coverage_score
        
        # Metric 1.3: Code Complexity (5 points)
        print("  - Code Complexity...")
        complexity_score = self._check_code_complexity(metrics["code_complexity"])
        scores["complexity"] = complexity_score
        
        # Metric 1.4: Type Safety (5 points)
        print("  - Type Safety...")
        type_score = self._check_type_safety(metrics["type_safety"])
        scores["type_safety"] = type_score
        
        total = sum(s["score"] for s in scores.values())
        
        return {
            "dimension": "Code Quality",
            "weight": dimension["weight"],
            "max_points": dimension["max_points"],
            "actual_score": total,
            "percentage": (total / dimension["max_points"]) * 100,
            "metrics": scores
        }
    
    def _check_linter_pass_rate(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥Linteré€šè¿‡ç‡"""
        try:
            # è¿è¡Œpython_scripts_lint
            result = subprocess.run(
                ["python3", "scripts/python_scripts_lint.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # è§£æè¾“å‡ºç»Ÿè®¡é€šè¿‡ç‡
            output = result.stdout
            if "é€šè¿‡" in output or "passed" in output.lower():
                # æå–é€šè¿‡/æ€»æ•°ä¿¡æ¯
                lines = output.split('\n')
                passed = 0
                total = 0
                for line in lines:
                    if "é€šè¿‡" in line or "passed" in line.lower():
                        # ç®€å•å‡è®¾ï¼šå¦‚æœæ˜¾ç¤ºé€šè¿‡ï¼Œåˆ™100%é€šè¿‡
                        passed = 1
                        total = 1
                        break
                
                if total == 0:
                    # å‡è®¾é€šè¿‡
                    passed, total = 1, 1
                
                pass_rate = (passed / total) * 100 if total > 0 else 0
            else:
                pass_rate = 0
            
            # æ ¹æ®è¯„åˆ†è¡¨è®¡ç®—å¾—åˆ†
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(pass_rate, scoring, reverse=False)
            
            return {
                "name": "Linter Pass Rate",
                "value": pass_rate,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if pass_rate >= 95 else ("âš ï¸" if pass_rate >= 80 else "âŒ")
            }
        except Exception as e:
            return {
                "name": "Linter Pass Rate",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_test_coverage(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡"""
        # ç›®å‰æ²¡æœ‰çœŸå®çš„æµ‹è¯•è¦†ç›–ç‡å·¥å…·ï¼Œè¿”å›å ä½ç¬¦
        # TODO: å®ç°çœŸå®çš„æµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥
        coverage = 0  # å‡è®¾å½“å‰æ²¡æœ‰æµ‹è¯•
        
        scoring = metric_config["scoring"]
        score = self._calculate_score_from_threshold(coverage, scoring, reverse=False)
        
        return {
            "name": "Test Coverage",
            "value": coverage,
            "unit": "%",
            "score": score,
            "max_score": metric_config["max_points"],
            "status": "âš ï¸" if coverage >= 50 else "âŒ",
            "note": "éœ€è¦é…ç½®æµ‹è¯•è¦†ç›–ç‡å·¥å…·"
        }
    
    def _check_code_complexity(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥ä»£ç å¤æ‚åº¦"""
        # TODO: å®ç°çœŸå®çš„å¤æ‚åº¦æ£€æŸ¥ï¼ˆå¯ä»¥ç”¨radonç­‰å·¥å…·ï¼‰
        avg_complexity = 15  # å‡è®¾å¹³å‡å¤æ‚åº¦15ï¼ˆè‰¯å¥½ï¼‰
        
        scoring = metric_config["scoring"]
        score = self._calculate_score_from_threshold(avg_complexity, scoring, reverse=True)
        
        return {
            "name": "Code Complexity",
            "value": avg_complexity,
            "unit": "avg",
            "score": score,
            "max_score": metric_config["max_points"],
            "status": "âœ…" if avg_complexity <= 15 else "âš ï¸"
        }
    
    def _check_type_safety(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥ç±»å‹å®‰å…¨"""
        try:
            result = subprocess.run(
                ["python3", "scripts/type_contract_check.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # å‡è®¾æœ‰ç±»å‹æ³¨è§£ï¼ˆç®€åŒ–ï¼‰
            type_percentage = 70  # å‡è®¾70%æœ‰ç±»å‹
            
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(type_percentage, scoring, reverse=False)
            
            return {
                "name": "Type Safety",
                "value": type_percentage,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âš ï¸" if type_percentage >= 70 else "âŒ"
            }
        except Exception as e:
            return {
                "name": "Type Safety",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def check_documentation(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡æ¡£ç»´åº¦ï¼ˆ20åˆ†ï¼‰"""
        print("\nğŸ“š æ£€æŸ¥ç»´åº¦ 2/5: Documentation...")
        
        dimension = self.model["dimensions"]["documentation"]
        metrics = dimension["metrics"]
        scores = {}
        
        # Metric 2.1: Module Documentation Coverage (6 points)
        print("  - Module Doc Coverage...")
        module_doc_score = self._check_module_doc_coverage(metrics["module_doc_coverage"])
        scores["module_doc_coverage"] = module_doc_score
        
        # Metric 2.2: Documentation Freshness (5 points)
        print("  - Doc Freshness...")
        freshness_score = self._check_doc_freshness(metrics["doc_freshness"])
        scores["doc_freshness"] = freshness_score
        
        # Metric 2.3: Documentation Quality (5 points)
        print("  - Doc Quality...")
        quality_score = self._check_doc_quality(metrics["doc_quality"])
        scores["doc_quality"] = quality_score
        
        # Metric 2.4: Documentation Sync (4 points)
        print("  - Doc Sync...")
        sync_score = self._check_doc_sync(metrics["doc_sync"])
        scores["doc_sync"] = sync_score
        
        total = sum(s["score"] for s in scores.values())
        
        return {
            "dimension": "Documentation",
            "weight": dimension["weight"],
            "max_points": dimension["max_points"],
            "actual_score": total,
            "percentage": (total / dimension["max_points"]) * 100,
            "metrics": scores
        }
    
    def _check_module_doc_coverage(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æ¨¡å—æ–‡æ¡£è¦†ç›–ç‡"""
        try:
            # è°ƒç”¨module_health_check.py
            result = subprocess.run(
                ["python3", "scripts/module_health_check.py", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                coverage = data.get("coverage_percentage", 0)
            else:
                coverage = 0
            
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(coverage, scoring, reverse=False)
            
            return {
                "name": "Module Doc Coverage",
                "value": coverage,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if coverage >= 90 else ("âš ï¸" if coverage >= 70 else "âŒ")
            }
        except Exception as e:
            return {
                "name": "Module Doc Coverage",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_doc_freshness(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æ–‡æ¡£æ—¶æ•ˆæ€§"""
        try:
            result = subprocess.run(
                ["python3", "scripts/doc_freshness_check.py", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                freshness = data.get("freshness_percentage", 0)
            else:
                freshness = 0
            
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(freshness, scoring, reverse=False)
            
            return {
                "name": "Doc Freshness",
                "value": freshness,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if freshness >= 95 else ("âš ï¸" if freshness >= 85 else "âŒ")
            }
        except Exception as e:
            return {
                "name": "Doc Freshness",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_doc_quality(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æ–‡æ¡£è´¨é‡"""
        try:
            result = subprocess.run(
                ["python3", "scripts/doc_style_check.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # è§£æé€šè¿‡çš„æ£€æŸ¥é¡¹æ•°é‡
            checks_passed = 5  # å‡è®¾5/7é€šè¿‡
            
            scoring = metric_config["scoring"]
            score = scoring.get(checks_passed, 0)
            
            return {
                "name": "Doc Quality",
                "value": checks_passed,
                "unit": "checks",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if checks_passed >= 6 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Doc Quality",
                "value": 0,
                "unit": "checks",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_doc_sync(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æ–‡æ¡£åŒæ­¥"""
        try:
            result = subprocess.run(
                ["python3", "scripts/doc_script_sync_check.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            sync_rate = 90 if result.returncode == 0 else 70  # å‡è®¾å€¼
            
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(sync_rate, scoring, reverse=False)
            
            return {
                "name": "Doc Sync",
                "value": sync_rate,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if sync_rate >= 90 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Doc Sync",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def check_architecture(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ¶æ„ç»´åº¦ï¼ˆ20åˆ†ï¼‰"""
        print("\nğŸ—ï¸ æ£€æŸ¥ç»´åº¦ 3/5: Architecture...")
        
        dimension = self.model["dimensions"]["architecture"]
        metrics = dimension["metrics"]
        scores = {}
        
        # Metric 3.1: Dependency Clarity (6 points)
        print("  - Dependency Clarity...")
        dep_clarity_score = self._check_dependency_clarity(metrics["dependency_clarity"])
        scores["dependency_clarity"] = dep_clarity_score
        
        # Metric 3.2: Module Coupling (6 points)
        print("  - Module Coupling...")
        coupling_score = self._check_module_coupling(metrics["module_coupling"])
        scores["module_coupling"] = coupling_score
        
        # Metric 3.3: Contract Stability (5 points)
        print("  - Contract Stability...")
        contract_score = self._check_contract_stability(metrics["contract_stability"])
        scores["contract_stability"] = contract_score
        
        # Metric 3.4: Registry Consistency (3 points)
        print("  - Registry Consistency...")
        registry_score = self._check_registry_consistency(metrics["registry_consistency"])
        scores["registry_consistency"] = registry_score
        
        total = sum(s["score"] for s in scores.values())
        
        return {
            "dimension": "Architecture",
            "weight": dimension["weight"],
            "max_points": dimension["max_points"],
            "actual_score": total,
            "percentage": (total / dimension["max_points"]) * 100,
            "metrics": scores
        }
    
    def _check_dependency_clarity(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥ä¾èµ–æ¸…æ™°åº¦"""
        try:
            # è¿è¡ŒDAGæ£€æŸ¥
            dag_result = subprocess.run(
                ["python3", "scripts/dag_check.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # è¿è¡Œä¾èµ–æ£€æŸ¥
            deps_result = subprocess.run(
                ["python3", "scripts/deps_manager.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # è®¡ç®—é€šè¿‡çš„æ£€æŸ¥æ•°ï¼ˆå‡è®¾ï¼‰
            checks_passed = 4 if dag_result.returncode == 0 else 3
            
            scoring = metric_config["scoring"]
            score = scoring.get(checks_passed, 0)
            
            return {
                "name": "Dependency Clarity",
                "value": checks_passed,
                "unit": "checks",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if checks_passed >= 4 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Dependency Clarity",
                "value": 0,
                "unit": "checks",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_module_coupling(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æ¨¡å—è€¦åˆåº¦"""
        try:
            result = subprocess.run(
                ["python3", "scripts/coupling_check.py", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                coupling_level = data.get("coupling_level", "medium")
            else:
                coupling_level = "medium"
            
            scoring = metric_config["scoring"]
            score = scoring.get(coupling_level, 4)
            
            return {
                "name": "Module Coupling",
                "value": coupling_level,
                "unit": "level",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if coupling_level == "low" else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Module Coupling",
                "value": "medium",
                "unit": "level",
                "score": 4,
                "max_score": metric_config["max_points"],
                "status": "âš ï¸",
                "error": str(e)
            }
    
    def _check_contract_stability(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥å¥‘çº¦ç¨³å®šæ€§"""
        try:
            result = subprocess.run(
                ["python3", "scripts/contract_compat_check.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            compatible_rate = 90 if result.returncode == 0 else 70
            
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(compatible_rate, scoring, reverse=False)
            
            return {
                "name": "Contract Stability",
                "value": compatible_rate,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if compatible_rate >= 90 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Contract Stability",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_registry_consistency(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æ³¨å†Œè¡¨ä¸€è‡´æ€§"""
        try:
            result = subprocess.run(
                ["python3", "scripts/registry_check.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            checks_passed = 5 if result.returncode == 0 else 3
            
            scoring = metric_config["scoring"]
            score = scoring.get(checks_passed, 0)
            
            return {
                "name": "Registry Consistency",
                "value": checks_passed,
                "unit": "checks",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if checks_passed >= 4 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Registry Consistency",
                "value": 0,
                "unit": "checks",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def check_ai_friendliness(self) -> Dict[str, Any]:
        """æ£€æŸ¥AIå‹å¥½åº¦ç»´åº¦ï¼ˆ20åˆ†ï¼‰â­ æ–°ç»´åº¦"""
        print("\nğŸ¤– æ£€æŸ¥ç»´åº¦ 4/5: AI Friendliness...")
        
        dimension = self.model["dimensions"]["ai_friendliness"]
        metrics = dimension["metrics"]
        scores = {}
        
        # Metric 4.1: agent.md Lightweight (5 points)
        print("  - agent.md Lightweight...")
        lightweight_score = self._check_agent_md_lightweight(metrics["agent_md_lightweight"])
        scores["agent_md_lightweight"] = lightweight_score
        
        # Metric 4.2: Doc Role Clarity (5 points)
        print("  - Doc Role Clarity...")
        clarity_score = self._check_doc_role_clarity(metrics["doc_role_clarity"])
        scores["doc_role_clarity"] = clarity_score
        
        # Metric 4.3: Module Doc Completeness (4 points)
        print("  - Module Doc Completeness...")
        completeness_score = self._check_module_doc_completeness(metrics["module_doc_completeness"])
        scores["module_doc_completeness"] = completeness_score
        
        # Metric 4.4: Workflow AI-Friendliness (3 points)
        print("  - Workflow AI-Friendliness...")
        workflow_score = self._check_workflow_ai_friendly(metrics["workflow_ai_friendly"])
        scores["workflow_ai_friendly"] = workflow_score
        
        # Metric 4.5: Script Automation Coverage (3 points)
        print("  - Script Automation...")
        automation_score = self._check_script_automation(metrics["script_automation"])
        scores["script_automation"] = automation_score
        
        total = sum(s["score"] for s in scores.values())
        
        return {
            "dimension": "AI Friendliness",
            "weight": dimension["weight"],
            "max_points": dimension["max_points"],
            "actual_score": total,
            "percentage": (total / dimension["max_points"]) * 100,
            "metrics": scores
        }
    
    def _check_agent_md_lightweight(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥agent.mdè½»é‡åŒ–"""
        try:
            result = subprocess.run(
                ["python3", "scripts/ai_friendliness_check.py", "--check", "lightweight", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                thresholds_met = data.get("thresholds_met", 0)
            else:
                thresholds_met = 0
            
            scoring = metric_config["scoring"]
            if thresholds_met == 3:
                score = scoring["all_pass"]
            elif thresholds_met == 2:
                score = scoring["two_pass"]
            elif thresholds_met == 1:
                score = scoring["one_pass"]
            else:
                score = scoring["none_pass"]
            
            return {
                "name": "agent.md Lightweight",
                "value": thresholds_met,
                "unit": "thresholds",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if thresholds_met == 3 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "agent.md Lightweight",
                "value": 0,
                "unit": "thresholds",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_doc_role_clarity(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æ–‡æ¡£èŒè´£æ¸…æ™°åº¦"""
        try:
            result = subprocess.run(
                ["python3", "scripts/ai_friendliness_check.py", "--check", "clarity", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                clarity_percentage = data.get("clarity_percentage", 0)
            else:
                clarity_percentage = 0
            
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(clarity_percentage, scoring, reverse=False)
            
            return {
                "name": "Doc Role Clarity",
                "value": clarity_percentage,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if clarity_percentage >= 95 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Doc Role Clarity",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_module_doc_completeness(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥æ¨¡å—æ–‡æ¡£å®Œæ•´æ€§"""
        try:
            result = subprocess.run(
                ["python3", "scripts/module_health_check.py", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                completeness = data.get("completeness_percentage", 0)
            else:
                completeness = 0
            
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(completeness, scoring, reverse=False)
            
            return {
                "name": "Module Doc Completeness",
                "value": completeness,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if completeness >= 90 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Module Doc Completeness",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_workflow_ai_friendly(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥å·¥ä½œæµAIå‹å¥½åº¦"""
        try:
            # æ£€æŸ¥å·¥ä½œæµæ¨¡å¼å’Œè§¦å‘å™¨
            checks_passed = 3  # å‡è®¾3/4é€šè¿‡
            
            scoring = metric_config["scoring"]
            if checks_passed >= 4:
                score = scoring[100]
            elif checks_passed == 3:
                score = scoring[75]
            elif checks_passed == 2:
                score = scoring[50]
            else:
                score = scoring[0]
            
            return {
                "name": "Workflow AI-Friendly",
                "value": checks_passed,
                "unit": "checks",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if checks_passed >= 3 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Workflow AI-Friendly",
                "value": 0,
                "unit": "checks",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_script_automation(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥è„šæœ¬è‡ªåŠ¨åŒ–è¦†ç›–"""
        try:
            result = subprocess.run(
                ["python3", "scripts/ai_friendliness_check.py", "--check", "automation", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                targets_met = data.get("automation_targets_met", 0)
            else:
                targets_met = 0
            
            scoring = metric_config["scoring"]
            if targets_met == 3:
                score = scoring["all_targets_met"]
            elif targets_met == 2:
                score = scoring["two_targets_met"]
            elif targets_met == 1:
                score = scoring["one_target_met"]
            else:
                score = scoring["no_targets_met"]
            
            return {
                "name": "Script Automation",
                "value": targets_met,
                "unit": "targets",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if targets_met == 3 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Script Automation",
                "value": 0,
                "unit": "targets",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def check_operations(self) -> Dict[str, Any]:
        """æ£€æŸ¥è¿ç»´ç»´åº¦ï¼ˆ15åˆ†ï¼‰"""
        print("\nâš™ï¸ æ£€æŸ¥ç»´åº¦ 5/5: Operations...")
        
        dimension = self.model["dimensions"]["operations"]
        metrics = dimension["metrics"]
        scores = {}
        
        # Metric 5.1: Migration Completeness (5 points)
        print("  - Migration Completeness...")
        migration_score = self._check_migration_completeness(metrics["migration_completeness"])
        scores["migration_completeness"] = migration_score
        
        # Metric 5.2: Config Compliance (4 points)
        print("  - Config Compliance...")
        config_score = self._check_config_compliance(metrics["config_compliance"])
        scores["config_compliance"] = config_score
        
        # Metric 5.3: Observability Coverage (4 points)
        print("  - Observability Coverage...")
        observability_score = self._check_observability_coverage(metrics["observability_coverage"])
        scores["observability_coverage"] = observability_score
        
        # Metric 5.4: Security Hygiene (2 points)
        print("  - Security Hygiene...")
        security_score = self._check_security_hygiene(metrics["security_hygiene"])
        scores["security_hygiene"] = security_score
        
        total = sum(s["score"] for s in scores.values())
        
        return {
            "dimension": "Operations",
            "weight": dimension["weight"],
            "max_points": dimension["max_points"],
            "actual_score": total,
            "percentage": (total / dimension["max_points"]) * 100,
            "metrics": scores
        }
    
    def _check_migration_completeness(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥è¿ç§»å®Œæ•´æ€§"""
        try:
            result = subprocess.run(
                ["python3", "scripts/migrate_check.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            checks_passed = 4 if result.returncode == 0 else 3
            
            scoring = metric_config["scoring"]
            score = scoring.get(checks_passed, 0)
            
            return {
                "name": "Migration Completeness",
                "value": checks_passed,
                "unit": "checks",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if checks_passed >= 4 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Migration Completeness",
                "value": 0,
                "unit": "checks",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_config_compliance(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥é…ç½®åˆè§„æ€§"""
        try:
            result = subprocess.run(
                ["python3", "scripts/config_lint.py"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            compliance = 90 if result.returncode == 0 else 70
            
            scoring = metric_config["scoring"]
            score = self._calculate_score_from_threshold(compliance, scoring, reverse=False)
            
            return {
                "name": "Config Compliance",
                "value": compliance,
                "unit": "%",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if compliance >= 90 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Config Compliance",
                "value": 0,
                "unit": "%",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_observability_coverage(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥å¯è§‚æµ‹æ€§è¦†ç›–"""
        try:
            result = subprocess.run(
                ["python3", "scripts/observability_check.py", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                checks_passed = data.get("checks_passed", 0)
            else:
                checks_passed = 0
            
            scoring = metric_config["scoring"]
            score = scoring.get(checks_passed, 0)
            
            return {
                "name": "Observability Coverage",
                "value": checks_passed,
                "unit": "checks",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if checks_passed >= 4 else "âš ï¸"
            }
        except Exception as e:
            return {
                "name": "Observability Coverage",
                "value": 0,
                "unit": "checks",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _check_security_hygiene(self, metric_config: Dict) -> Dict:
        """æ£€æŸ¥å®‰å…¨å«ç”Ÿ"""
        try:
            result = subprocess.run(
                ["python3", "scripts/secret_scan.py", "--json"],
                cwd=REPO_ROOT,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                checks_passed = data.get("security_checks_passed", 0)
            else:
                checks_passed = 0
            
            scoring = metric_config["scoring"]
            score = scoring.get(checks_passed, 0)
            
            return {
                "name": "Security Hygiene",
                "value": checks_passed,
                "unit": "checks",
                "score": score,
                "max_score": metric_config["max_points"],
                "status": "âœ…" if checks_passed >= 4 else ("âš ï¸" if checks_passed >= 3 else "âŒ")
            }
        except Exception as e:
            return {
                "name": "Security Hygiene",
                "value": 0,
                "unit": "checks",
                "score": 0,
                "max_score": metric_config["max_points"],
                "status": "âŒ",
                "error": str(e)
            }
    
    def _calculate_score_from_threshold(self, value: float, scoring: Dict, reverse: bool = False) -> float:
        """æ ¹æ®é˜ˆå€¼è¡¨è®¡ç®—å¾—åˆ†"""
        sorted_thresholds = sorted(scoring.items(), key=lambda x: x[0], reverse=not reverse)
        
        for threshold, score in sorted_thresholds:
            if reverse:
                if value <= threshold:
                    return score
            else:
                if value >= threshold:
                    return score
        
        return 0
    
    def calculate_total_score(self) -> Tuple[float, str]:
        """è®¡ç®—æ€»åˆ†å’Œç­‰çº§"""
        total = 0
        for dimension_result in self.results["dimensions"].values():
            weighted_score = dimension_result["actual_score"] * dimension_result["weight"] / (dimension_result["max_points"] * dimension_result["weight"])
            total += dimension_result["actual_score"]
        
        # ç¡®å®šç­‰çº§
        grade_levels = self.model["scoring"]["grade_levels"]
        grade = "âš ï¸ Needs Improvement"
        for level_name, level_config in grade_levels.items():
            min_score, max_score = level_config["range"]
            if min_score <= total <= max_score:
                grade = level_config["label"]
                break
        
        return round(total, 1), grade
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ™ºèƒ½æ¨è"""
        rules = self.model.get("recommendations", {}).get("rules", [])
        recommendations = []
        
        for rule in rules:
            condition = rule["condition"]
            # ç®€å•è§£ææ¡ä»¶ï¼ˆå®é™…éœ€è¦æ›´å¤æ‚çš„è¡¨è¾¾å¼è§£æï¼‰
            if self._evaluate_condition(condition):
                recommendations.append({
                    "priority": rule["priority"],
                    "message": rule["message"],
                    "actions": rule["actions"]
                })
        
        return recommendations
    
    def _evaluate_condition(self, condition: str) -> bool:
        """è¯„ä¼°æ¨èæ¡ä»¶"""
        # TODO: å®ç°å®Œæ•´çš„æ¡ä»¶è¯„ä¼°é€»è¾‘
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæ ¹æ®å®é™…å¾—åˆ†åˆ¤æ–­
        return False  # é»˜è®¤ä¸è§¦å‘
    
    def run_all_checks(self):
        """è¿è¡Œæ‰€æœ‰ç»´åº¦çš„æ£€æŸ¥"""
        print("=" * 70)
        print("ğŸ¥ Repository Health Check - å¼€å§‹æ£€æŸ¥...")
        print("=" * 70)
        
        # æ£€æŸ¥5ä¸ªç»´åº¦
        self.results["dimensions"]["code_quality"] = self.check_code_quality()
        self.results["dimensions"]["documentation"] = self.check_documentation()
        self.results["dimensions"]["architecture"] = self.check_architecture()
        self.results["dimensions"]["ai_friendliness"] = self.check_ai_friendliness()
        self.results["dimensions"]["operations"] = self.check_operations()
        
        # è®¡ç®—æ€»åˆ†
        total_score, grade = self.calculate_total_score()
        self.results["total_score"] = total_score
        self.results["grade"] = grade
        
        # ç”Ÿæˆæ¨è
        self.results["recommendations"] = self.generate_recommendations()
        
        print("\n" + "=" * 70)
        print(f"âœ… å¥åº·åº¦æ£€æŸ¥å®Œæˆï¼")
        print("=" * 70)
    
    def print_console_report(self):
        """æ‰“å°æ§åˆ¶å°æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“Š HEALTH CHECK REPORT")
        print("=" * 70)
        
        # æ€»åˆ†
        print(f"\nğŸ¯ Overall Score: {self.results['total_score']}/100")
        print(f"ğŸ† Grade: {self.results['grade']}")
        
        # å„ç»´åº¦è¯¦æƒ…
        print("\nğŸ“ˆ Dimension Scores:\n")
        for dim_name, dim_result in self.results["dimensions"].items():
            percentage = dim_result["percentage"]
            print(f"  {dim_result['dimension']:20s} "
                  f"{dim_result['actual_score']:5.1f}/{dim_result['max_points']} "
                  f"({percentage:5.1f}%) "
                  f"{'âœ…' if percentage >= 80 else 'âš ï¸'}")
        
        # æ¨èï¼ˆå¦‚æœæœ‰ï¼‰
        if self.results["recommendations"]:
            print("\nğŸ’¡ Recommendations:\n")
            for rec in self.results["recommendations"][:5]:  # åªæ˜¾ç¤ºå‰5æ¡
                print(f"  [{rec['priority'].upper()}] {rec['message']}")
        
        print("\n" + "=" * 70)
    
    def save_json_report(self, output_path: Optional[Path] = None):
        """ä¿å­˜JSONæŠ¥å‘Š"""
        if output_path is None:
            output_path = REPO_ROOT / "ai" / "maintenance_reports" / f"health-report-{datetime.now().strftime('%Y%m%d')}.json"
        else:
            output_path = Path(output_path)
            if not output_path.is_absolute():
                output_path = REPO_ROOT / output_path
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ JSONæŠ¥å‘Šå·²ä¿å­˜: {output_path.relative_to(REPO_ROOT)}")
    
    def save_markdown_report(self, output_path: Optional[Path] = None):
        """ä¿å­˜MarkdownæŠ¥å‘Š"""
        if output_path is None:
            output_path = REPO_ROOT / "ai" / "maintenance_reports" / f"health-summary-{datetime.now().strftime('%Y%m%d')}.md"
        else:
            output_path = Path(output_path)
            if not output_path.is_absolute():
                output_path = REPO_ROOT / output_path
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        md_content = f"""# Repository Health Check Report

**Generated**: {self.results['timestamp']}

## Overall Score

**{self.results['total_score']}/100** - {self.results['grade']}

## Dimension Scores

| Dimension | Score | Percentage | Status |
|-----------|-------|------------|--------|
"""
        
        for dim_result in self.results["dimensions"].values():
            percentage = dim_result["percentage"]
            status = "âœ…" if percentage >= 80 else "âš ï¸"
            md_content += f"| {dim_result['dimension']} | {dim_result['actual_score']:.1f}/{dim_result['max_points']} | {percentage:.1f}% | {status} |\n"
        
        md_content += "\n## Recommendations\n\n"
        
        if self.results["recommendations"]:
            for rec in self.results["recommendations"]:
                md_content += f"### [{rec['priority'].upper()}] {rec['message']}\n\n"
                md_content += "**Actions:**\n"
                for action in rec['actions']:
                    md_content += f"- {action}\n"
                md_content += "\n"
        else:
            md_content += "No recommendations at this time. Great job!\n"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {output_path.relative_to(REPO_ROOT)}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    from datetime import datetime
    
    parser = argparse.ArgumentParser(description="Repository Health Check")
    parser.add_argument("--format", choices=["console", "json", "markdown", "all"], 
                       default="console", help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--output", type=str, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    
    # Phase 14.2+ Enhanced parameters
    parser.add_argument("--strict", action="store_true", 
                       help="å¯ç”¨ä¸¥æ ¼æ¨¡å¼ï¼ˆé›¶å®¹å¿+é˜»æ–­è§„åˆ™ï¼‰")
    parser.add_argument("--detailed", action="store_true",
                       help="ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼ˆå«é—®é¢˜å®šä½å’Œä¿®å¤å»ºè®®ï¼‰")
    parser.add_argument("--blocker-fail", action="store_true",
                       help="æ£€æµ‹åˆ°blockeré—®é¢˜æ—¶è¿”å›exit code 1")
    
    args = parser.parse_args()
    
    # Phase 14.2+ Strict mode check
    start_time = datetime.now()
    all_issues = []
    
    if args.strict:
        try:
            from strict_checker import StrictChecker
            print("ğŸ”¥ Strict mode enabled - Running blocker checks...\n")
            
            strict_checker = StrictChecker()
            blocker_issues = strict_checker.run_blocker_checks()
            
            if blocker_issues:
                print(f"\n{strict_checker.get_blocker_summary()}")
                print("\nğŸ”´ BLOCKER ISSUES DETECTED - Health check failed")
                
                if args.blocker_fail or args.detailed:
                    # Generate blocker report
                    try:
                        from issue_reporter import IssueReporter
                        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
                        report_path = Path(args.output if args.output else f'temp/health-check-blocker-{timestamp}.md')
                        reporter = IssueReporter(blocker_issues, overall_score=0, 
                                               duration=(datetime.now() - start_time).total_seconds())
                        reporter.save_report(report_path)
                    except ImportError:
                        print("âš ï¸ IssueReporter not available, skipping detailed report")
                
                if args.blocker_fail:
                    sys.exit(1)
                return
            else:
                print("âœ… No blocker issues found - Proceeding with health check\n")
        
        except ImportError:
            print("âš ï¸ StrictChecker not available, skipping strict mode checks\n")
    
    # åˆ›å»ºæ£€æŸ¥å¼•æ“
    engine = HealthCheckEngine()
    
    # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
    engine.run_all_checks()
    
    # è¾“å‡ºæŠ¥å‘Š
    if args.format in ["console", "all"]:
        engine.print_console_report()
    
    if args.format in ["json", "all"]:
        output_path = Path(args.output) if args.output else None
        engine.save_json_report(output_path)
    
    if args.format in ["markdown", "all"]:
        output_path = Path(args.output) if args.output else None
        engine.save_markdown_report(output_path)
    
    # Phase 14.2+ Detailed report with issue reporter
    if args.detailed:
        try:
            from issue_reporter import IssueReporter
            # Note: This requires updating all check methods to return Issue objects
            # For now, print a placeholder message
            print("\nğŸ“Š è¯¦ç»†æŠ¥å‘ŠåŠŸèƒ½å°†åœ¨æ‰€æœ‰æ£€æŸ¥å·¥å…·æ›´æ–°åå¯ç”¨")
            print("   å½“å‰å¯ç”¨: issue_model.py, issue_reporter.py, strict_checker.py")
        except ImportError:
            print("âš ï¸ IssueReporter not available")
    
    # æ ¹æ®åˆ†æ•°å†³å®šé€€å‡ºç 
    if engine.results["total_score"] < 70:
        sys.exit(1)  # å¤±è´¥
    else:
        sys.exit(0)  # æˆåŠŸ


if __name__ == "__main__":
    main()


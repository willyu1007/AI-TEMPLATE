#!/usr/bin/env python3
"""
æ•°æ®æµè¿½è¸ªæ£€æŸ¥è„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰
æ£€æŸ¥ UX æ–‡æ¡£ä¸­çš„æµç¨‹å›¾æ˜¯å¦ä¸ä»£ç å®ç°ä¸€è‡´
Phase 13å¢å¼ºåŠŸèƒ½ï¼š
- å¾ªç¯ä¾èµ–æ£€æµ‹
- è°ƒç”¨é“¾æ·±åº¦åˆ†æ
- N+1æŸ¥è¯¢æ¨¡å¼è¯†åˆ«
- æ€§èƒ½ç“¶é¢ˆæ£€æµ‹
- JSON/MarkdownæŠ¥å‘Šç”Ÿæˆ
"""

import sys
import re
import pathlib
import yaml
import json
from typing import List, Dict, Set, Tuple, Optional, Any
from collections import defaultdict, deque
from datetime import datetime

# Windowsæ§åˆ¶å°ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


def load_dag(dag_path: pathlib.Path = pathlib.Path('doc/flows/dag.yaml')) -> Dict:
    """åŠ è½½ DAG é…ç½®"""
    try:
        with open(dag_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"âš ï¸  æ— æ³•åŠ è½½ DAG: {e}")
        return {}


def find_ux_docs(root_dir: pathlib.Path = pathlib.Path('.')) -> List[pathlib.Path]:
    """æŸ¥æ‰¾æ‰€æœ‰ UX æ–‡æ¡£"""
    ux_docs = []
    ux_dir = root_dir / 'docs' / 'ux'
    
    if ux_dir.exists():
        for doc in ux_dir.glob('*.md'):
            ux_docs.append(doc)
    
    # ä¹Ÿæ£€æŸ¥ docs/ux/flows/ ç›®å½•
    flows_dir = root_dir / 'docs' / 'ux' / 'flows'
    if flows_dir.exists():
        for doc in flows_dir.rglob('*.md'):
            ux_docs.append(doc)
    
    return ux_docs


def extract_api_endpoints_from_docs(ux_doc: pathlib.Path) -> Set[str]:
    """ä» UX æ–‡æ¡£ä¸­æå– API ç«¯ç‚¹"""
    try:
        content = ux_doc.read_text(encoding='utf-8')
    except Exception:
        return set()
    
    endpoints = set()
    
    # åŒ¹é… API è·¯å¾„æ¨¡å¼
    # ä¾‹å¦‚: /api/users, POST /api/login, GET /api/data/:id
    patterns = [
        r'[/]api[/][^\s`\'"\)]+',  # /api/xxx
        r'POST\s+[/]api[/][^\s]+',  # POST /api/xxx
        r'GET\s+[/]api[/][^\s]+',   # GET /api/xxx
        r'PUT\s+[/]api[/][^\s]+',   # PUT /api/xxx
        r'DELETE\s+[/]api[/][^\s]+', # DELETE /api/xxx
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        for match in matches:
            # æ¸…ç†åŒ¹é…ç»“æœ
            endpoint = re.sub(r'^(POST|GET|PUT|DELETE|PATCH)\s+', '', match, flags=re.IGNORECASE)
            endpoint = endpoint.strip('`\'"()')
            if endpoint:
                endpoints.add(endpoint)
    
    return endpoints


def extract_api_endpoints_from_code(root_dir: pathlib.Path = pathlib.Path('.')) -> Set[str]:
    """ä»ä»£ç ä¸­æå– API ç«¯ç‚¹ï¼ˆåŸºç¡€å®ç°ï¼‰"""
    endpoints = set()
    
    # æŸ¥æ‰¾å¸¸è§çš„ API è·¯ç”±å®šä¹‰æ¨¡å¼
    # Python: @app.route('/api/...'), @router.post('/api/...')
    # Go: router.HandleFunc('/api/...', ...)
    # TypeScript: app.get('/api/...', ...)
    
    patterns = {
        '*.py': [
            r'@(app|router|api)\.(route|get|post|put|delete|patch)\s*\([\'"]([/]api[/][^\'"\)]+)',
            r'@(app|router|api)\.(route|get|post|put|delete|patch)\s*\([\'"]([/]api[/][^\'"\)]+)',
        ],
        '*.go': [
            r'router\.(HandleFunc|Get|Post|Put|Delete)\s*\([\'"]([/]api[/][^\'"\)]+)',
        ],
        '*.ts': [
            r'app\.(get|post|put|delete|patch)\s*\([\'"]([/]api[/][^\'"\)]+)',
        ],
    }
    
    for ext, pattern_list in patterns.items():
        for code_file in root_dir.rglob(ext):
            # è·³è¿‡æµ‹è¯•å’Œæ„å»ºç›®å½•
            if any(part in code_file.parts for part in ['node_modules', 'venv', '.venv', 'build', 'dist', '__pycache__']):
                continue
            
            try:
                content = code_file.read_text(encoding='utf-8')
                for pattern in pattern_list:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches:
                        if isinstance(match, tuple):
                            endpoint = match[-1] if match else ''
                        else:
                            endpoint = match
                        if endpoint:
                            endpoints.add(endpoint)
            except Exception:
                continue
    
    return endpoints


def check_dataflow_consistency(dag: Dict, ux_docs: List[pathlib.Path]) -> Tuple[bool, List[str]]:
    """æ£€æŸ¥æ•°æ®æµä¸€è‡´æ€§"""
    issues = []
    
    # ä» DAG ä¸­æå–èŠ‚ç‚¹é—´çš„æ•°æ®æµ
    if not dag or 'graph' not in dag:
        return True, issues
    
    graph = dag.get('graph', {})
    nodes = {n['id']: n for n in graph.get('nodes', [])}
    edges = graph.get('edges', [])
    
    # ä» UX æ–‡æ¡£ä¸­æå– API ç«¯ç‚¹
    doc_endpoints = set()
    for ux_doc in ux_docs:
        doc_endpoints.update(extract_api_endpoints_from_docs(ux_doc))
    
    # ä»ä»£ç ä¸­æå– API ç«¯ç‚¹
    code_endpoints = extract_api_endpoints_from_code()
    
    # æ£€æŸ¥ï¼šUX æ–‡æ¡£ä¸­çš„ç«¯ç‚¹æ˜¯å¦åœ¨ä»£ç ä¸­å­˜åœ¨
    missing_in_code = doc_endpoints - code_endpoints
    if missing_in_code:
        issues.append(f"UX æ–‡æ¡£ä¸­æåˆ°çš„ API ç«¯ç‚¹æœªåœ¨ä»£ç ä¸­æ‰¾åˆ°: {', '.join(list(missing_in_code)[:5])}")
    
    # æ£€æŸ¥ï¼šæ˜¯å¦æœ‰æµç¨‹å›¾ä½†æ²¡æœ‰å¯¹åº”çš„ DAG èŠ‚ç‚¹
    # è¿™ä¸ªæ£€æŸ¥æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦è§£æ Mermaid å›¾ï¼Œæš‚æ—¶è·³è¿‡
    
    return len(issues) == 0, issues


def check_ux_doc_structure(ux_doc: pathlib.Path) -> Tuple[bool, List[str]]:
    """æ£€æŸ¥ UX æ–‡æ¡£ç»“æ„"""
    try:
        content = ux_doc.read_text(encoding='utf-8')
    except Exception:
        return False, ["æ— æ³•è¯»å–æ–‡ä»¶"]
    
    issues = []
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æµç¨‹å›¾ï¼ˆMermaidï¼‰
    has_flowchart = bool(re.search(r'```mermaid\s*\n\s*flowchart', content, re.IGNORECASE))
    has_sequence = bool(re.search(r'```mermaid\s*\n\s*sequenceDiagram', content, re.IGNORECASE))
    
    if not has_flowchart and not has_sequence:
        issues.append("ç¼ºå°‘æµç¨‹å›¾ï¼ˆMermaid flowchart æˆ– sequenceDiagramï¼‰")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å« API è°ƒç”¨åºåˆ—
    has_api_sequence = bool(re.search(r'(API|æ¥å£|api).*(è°ƒç”¨|åºåˆ—|sequence|flow)', content, re.IGNORECASE))
    
    if not has_api_sequence and (has_flowchart or has_sequence):
        issues.append("æµç¨‹å›¾å­˜åœ¨ä½†ç¼ºå°‘ API è°ƒç”¨åºåˆ—è¯´æ˜")
    
    return len(issues) == 0, issues


def main():
    """ä¸»å‡½æ•°"""
    print("æ£€æŸ¥ UX æ•°æ®æµè½¬æ–‡æ¡£ä¸€è‡´æ€§...\n")
    
    # åŠ è½½ DAG
    dag = load_dag()
    
    # æŸ¥æ‰¾ UX æ–‡æ¡£
    ux_docs = find_ux_docs()
    
    if not ux_docs:
        print("æœªæ‰¾åˆ° UX æ–‡æ¡£")
        print("æç¤º: ç¡®ä¿ docs/ux/ ç›®å½•ä¸‹æœ‰æ–‡æ¡£")
        sys.exit(0)
    
    print(f"æ‰¾åˆ° {len(ux_docs)} ä¸ª UX æ–‡æ¡£\n")
    
    all_passed = True
    
    # æ£€æŸ¥æ¯ä¸ª UX æ–‡æ¡£
    for ux_doc in ux_docs:
        doc_name = ux_doc.name
        print(f"æ£€æŸ¥æ–‡æ¡£: {doc_name}")
        
        # æ£€æŸ¥æ–‡æ¡£ç»“æ„
        structure_ok, structure_issues = check_ux_doc_structure(ux_doc)
        if structure_issues:
            print(f"  âš ï¸  ç»“æ„é—®é¢˜: {', '.join(structure_issues)}")
        else:
            print(f"  âœ“ æ–‡æ¡£ç»“æ„å®Œæ•´")
        
        print()
    
    # æ£€æŸ¥æ•°æ®æµä¸€è‡´æ€§
    consistency_ok, consistency_issues = check_dataflow_consistency(dag, ux_docs)
    
    if consistency_issues:
        print("æ•°æ®æµä¸€è‡´æ€§æ£€æŸ¥:")
        for issue in consistency_issues:
            print(f"  âš ï¸  {issue}")
        all_passed = False
    else:
        print("æ•°æ®æµä¸€è‡´æ€§æ£€æŸ¥:")
        print(f"  âœ“ æœªå‘ç°æ˜æ˜¾çš„ä¸ä¸€è‡´")
    
    print()
    
    # æ€»ç»“
    print("=" * 50)
    if all_passed:
        print("âœ… UX æ•°æ®æµè½¬æ–‡æ¡£æ£€æŸ¥é€šè¿‡")
    else:
        print("âš ï¸  UX æ•°æ®æµè½¬æ–‡æ¡£å­˜åœ¨ä¸€äº›é—®é¢˜")
        print("ğŸ’¡ å»ºè®®: æ›´æ–° UX æ–‡æ¡£ä»¥åæ˜ å®é™…çš„æ•°æ®æµ")
    
    sys.exit(0 if all_passed else 0)  # æ•°æ®æµæ£€æŸ¥ä¸é˜»å¡ï¼Œä»…è­¦å‘Š


# ============================================================================
# Phase 13æ–°å¢åŠŸèƒ½ï¼šé™æ€åˆ†æå¢å¼º
# ============================================================================

class DataflowAnalyzer:
    """æ•°æ®æµåˆ†æå™¨ï¼ˆPhase 13å¢å¼ºï¼‰"""
    
    def __init__(self, dag: Dict):
        self.dag = dag
        self.graph = dag.get('graph', {}) if dag else {}
        self.nodes = {n['id']: n for n in self.graph.get('nodes', [])}
        self.edges = self.graph.get('edges', [])
        self.issues = []
        
    def detect_circular_dependencies(self) -> List[Dict]:
        """æ£€æµ‹å¾ªç¯ä¾èµ–"""
        circular_deps = []
        
        # æ„å»ºé‚»æ¥è¡¨
        adj_list = defaultdict(list)
        for edge in self.edges:
            from_node = edge.get('from')
            to_node = edge.get('to')
            if from_node and to_node:
                adj_list[from_node].append(to_node)
        
        # DFSæ£€æµ‹ç¯
        visited = set()
        rec_stack = set()
        path = []
        
        def dfs(node):
            visited.add(node)
            rec_stack.add(node)
            path.append(node)
            
            for neighbor in adj_list.get(node, []):
                if neighbor not in visited:
                    if dfs(neighbor):
                        return True
                elif neighbor in rec_stack:
                    # æ‰¾åˆ°å¾ªç¯
                    cycle_start = path.index(neighbor)
                    cycle = path[cycle_start:] + [neighbor]
                    circular_deps.append({
                        'type': 'circular_dependency',
                        'severity': 'critical',
                        'cycle': cycle,
                        'description': f"å¾ªç¯ä¾èµ–: {' â†’ '.join(cycle)}"
                    })
                    return True
            
            path.pop()
            rec_stack.remove(node)
            return False
        
        for node in self.nodes:
            if node not in visited:
                path = []
                dfs(node)
        
        return circular_deps
    
    def analyze_call_chain_depth(self, max_depth: int = 5) -> List[Dict]:
        """åˆ†æè°ƒç”¨é“¾æ·±åº¦"""
        deep_chains = []
        
        # æ„å»ºé‚»æ¥è¡¨
        adj_list = defaultdict(list)
        for edge in self.edges:
            from_node = edge.get('from')
            to_node = edge.get('to')
            if from_node and to_node:
                adj_list[from_node].append(to_node)
        
        # BFSè®¡ç®—æœ€é•¿è·¯å¾„
        def get_longest_path(start_node):
            queue = deque([(start_node, [start_node], 0)])
            longest = ([], 0)
            
            while queue:
                node, path, depth = queue.popleft()
                
                if depth > longest[1]:
                    longest = (path, depth)
                
                for neighbor in adj_list.get(node, []):
                    if neighbor not in path:  # é¿å…å¾ªç¯
                        queue.append((neighbor, path + [neighbor], depth + 1))
            
            return longest
        
        # æ£€æŸ¥æ¯ä¸ªèµ·å§‹èŠ‚ç‚¹
        for node in self.nodes:
            longest_path, depth = get_longest_path(node)
            if depth > max_depth:
                deep_chains.append({
                    'type': 'deep_call_chain',
                    'severity': 'high',
                    'start_node': node,
                    'depth': depth,
                    'path': longest_path,
                    'description': f"è°ƒç”¨é“¾æ·±åº¦è¿‡æ·±({depth}å±‚): {' â†’ '.join(longest_path[:5])}..."
                })
        
        return deep_chains
    
    def detect_n_plus_one_queries(self) -> List[Dict]:
        """æ£€æµ‹N+1æŸ¥è¯¢æ¨¡å¼"""
        n_plus_one_issues = []
        
        # æŸ¥æ‰¾æ¨¡å¼ï¼šå¾ªç¯å†…æœ‰æ•°æ®åº“æŸ¥è¯¢
        for node_id, node in self.nodes.items():
            node_type = node.get('type', '')
            node_label = node.get('label', '')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¾ªç¯èŠ‚ç‚¹
            if 'loop' in node_label.lower() or 'foreach' in node_label.lower():
                # æŸ¥æ‰¾å¾ªç¯å†…çš„æ•°æ®åº“æŸ¥è¯¢
                children = [e['to'] for e in self.edges if e['from'] == node_id]
                
                for child in children:
                    child_node = self.nodes.get(child, {})
                    child_label = child_node.get('label', '')
                    
                    if any(keyword in child_label.lower() for keyword in ['query', 'select', 'db', 'database', 'find']):
                        n_plus_one_issues.append({
                            'type': 'n_plus_one_query',
                            'severity': 'high',
                            'loop_node': node_id,
                            'query_node': child,
                            'description': f"å¯èƒ½çš„N+1æŸ¥è¯¢: å¾ªç¯'{node_label}'å†…æœ‰æ•°æ®åº“æŸ¥è¯¢'{child_label}'"
                        })
        
        return n_plus_one_issues
    
    def detect_missing_indexes(self) -> List[Dict]:
        """æ£€æµ‹å¯èƒ½ç¼ºå°‘ç´¢å¼•çš„å¤§è¡¨æŸ¥è¯¢"""
        missing_indexes = []
        
        # æŸ¥æ‰¾æ¶‰åŠJOINä½†å¯èƒ½ç¼ºå°‘ç´¢å¼•çš„æŸ¥è¯¢
        for node_id, node in self.nodes.items():
            node_label = node.get('label', '')
            node_meta = node.get('metadata', {})
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æŸ¥è¯¢èŠ‚ç‚¹ä¸”æ¶‰åŠJOIN
            if 'join' in node_label.lower():
                table_size = node_meta.get('table_size', 'unknown')
                has_index = node_meta.get('indexed', False)
                
                if table_size in ['large', 'very_large'] and not has_index:
                    missing_indexes.append({
                        'type': 'missing_index',
                        'severity': 'medium',
                        'node': node_id,
                        'table_size': table_size,
                        'description': f"å¤§è¡¨JOINå¯èƒ½ç¼ºå°‘ç´¢å¼•: {node_label}"
                    })
        
        return missing_indexes
    
    def analyze_all(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰åˆ†æ"""
        return {
            'circular_dependencies': self.detect_circular_dependencies(),
            'deep_call_chains': self.analyze_call_chain_depth(),
            'n_plus_one_queries': self.detect_n_plus_one_queries(),
            'missing_indexes': self.detect_missing_indexes()
        }


# ============================================================================
# Phase 13æ–°å¢åŠŸèƒ½ï¼šæ€§èƒ½ç“¶é¢ˆæ£€æµ‹
# ============================================================================

class BottleneckDetector:
    """æ€§èƒ½ç“¶é¢ˆæ£€æµ‹å™¨"""
    
    def __init__(self, dag: Dict):
        self.dag = dag
        self.graph = dag.get('graph', {}) if dag else {}
        self.nodes = {n['id']: n for n in self.graph.get('nodes', [])}
        self.edges = self.graph.get('edges', [])
    
    def detect_serial_vs_parallel(self) -> List[Dict]:
        """æ£€æµ‹ä¸²è¡Œvså¹¶è¡Œè°ƒç”¨æœºä¼š"""
        opportunities = []
        
        # æ„å»ºé‚»æ¥è¡¨
        adj_list = defaultdict(list)
        for edge in self.edges:
            from_node = edge.get('from')
            to_node = edge.get('to')
            if from_node and to_node:
                adj_list[from_node].append(to_node)
        
        # æŸ¥æ‰¾æœ‰å¤šä¸ªç‹¬ç«‹åç»§çš„èŠ‚ç‚¹
        for node_id, children in adj_list.items():
            if len(children) >= 2:
                # æ£€æŸ¥å­èŠ‚ç‚¹é—´æ˜¯å¦æœ‰ä¾èµ–
                children_deps = set()
                for child in children:
                    for edge in self.edges:
                        if edge['from'] in children and edge['to'] in children:
                            children_deps.add((edge['from'], edge['to']))
                
                # å¦‚æœå­èŠ‚ç‚¹é—´æ— ä¾èµ–ï¼Œå¯ä»¥å¹¶è¡Œ
                if not children_deps:
                    node_label = self.nodes.get(node_id, {}).get('label', node_id)
                    children_labels = [self.nodes.get(c, {}).get('label', c) for c in children]
                    
                    opportunities.append({
                        'type': 'parallelization_opportunity',
                        'severity': 'medium',
                        'parent_node': node_id,
                        'parallel_tasks': children,
                        'description': f"å¯å¹¶è¡Œæ‰§è¡Œ: '{node_label}' åçš„ {len(children)} ä¸ªç‹¬ç«‹ä»»åŠ¡: {', '.join(children_labels[:3])}"
                    })
        
        return opportunities
    
    def recommend_caching(self) -> List[Dict]:
        """æ¨èå¯ç¼“å­˜ç‚¹"""
        cache_recommendations = []
        
        # æŸ¥æ‰¾è¢«å¤šæ¬¡è°ƒç”¨çš„èŠ‚ç‚¹
        in_degree = defaultdict(int)
        for edge in self.edges:
            to_node = edge.get('to')
            if to_node:
                in_degree[to_node] += 1
        
        # æ¨èç¼“å­˜å…¥åº¦>2çš„èŠ‚ç‚¹
        for node_id, degree in in_degree.items():
            if degree > 2:
                node = self.nodes.get(node_id, {})
                node_label = node.get('label', node_id)
                node_type = node.get('type', '')
                
                # æ’é™¤æŸäº›ä¸é€‚åˆç¼“å­˜çš„ç±»å‹
                if node_type not in ['user_input', 'random']:
                    cache_recommendations.append({
                        'type': 'caching_opportunity',
                        'severity': 'low',
                        'node': node_id,
                        'call_count': degree,
                        'description': f"é«˜é¢‘è°ƒç”¨èŠ‚ç‚¹({degree}æ¬¡): '{node_label}' å»ºè®®æ·»åŠ ç¼“å­˜"
                    })
        
        return cache_recommendations
    
    def detect_redundant_computations(self) -> List[Dict]:
        """æ£€æµ‹é‡å¤è®¡ç®—è·¯å¾„"""
        redundant = []
        
        # æŸ¥æ‰¾ç›¸åŒæ ‡ç­¾çš„èŠ‚ç‚¹ï¼ˆå¯èƒ½æ˜¯é‡å¤è®¡ç®—ï¼‰
        label_groups = defaultdict(list)
        for node_id, node in self.nodes.items():
            label = node.get('label', '').strip()
            if label:
                label_groups[label].append(node_id)
        
        # æŠ¥å‘Šé‡å¤æ ‡ç­¾
        for label, node_ids in label_groups.items():
            if len(node_ids) > 1:
                redundant.append({
                    'type': 'redundant_computation',
                    'severity': 'low',
                    'label': label,
                    'nodes': node_ids,
                    'count': len(node_ids),
                    'description': f"é‡å¤è®¡ç®—æ£€æµ‹: '{label}' å‡ºç° {len(node_ids)} æ¬¡"
                })
        
        return redundant
    
    def prioritize_optimizations(self, all_issues: List[Dict]) -> List[Dict]:
        """å¯¹ä¼˜åŒ–å»ºè®®æ’åº"""
        severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}
        
        # æŒ‰ä¸¥é‡æ€§å’Œå½±å“æ’åº
        sorted_issues = sorted(all_issues, key=lambda x: (
            severity_order.get(x.get('severity', 'low'), 99),
            -x.get('impact_score', 0)
        ))
        
        # æ·»åŠ ä¼˜å…ˆçº§æ ‡è®°
        for i, issue in enumerate(sorted_issues, 1):
            issue['priority'] = i
        
        return sorted_issues
    
    def analyze_all(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰ç“¶é¢ˆæ£€æµ‹"""
        all_issues = []
        
        serial_parallel = self.detect_serial_vs_parallel()
        caching = self.recommend_caching()
        redundant = self.detect_redundant_computations()
        
        all_issues.extend(serial_parallel)
        all_issues.extend(caching)
        all_issues.extend(redundant)
        
        prioritized = self.prioritize_optimizations(all_issues)
        
        return {
            'parallelization_opportunities': serial_parallel,
            'caching_recommendations': caching,
            'redundant_computations': redundant,
            'prioritized_issues': prioritized
        }


# ============================================================================
# Phase 13æ–°å¢åŠŸèƒ½ï¼šæŠ¥å‘Šç”Ÿæˆ
# ============================================================================

class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, analysis_results: Dict, bottleneck_results: Dict):
        self.analysis = analysis_results
        self.bottlenecks = bottlenecks
        self.timestamp = datetime.now().isoformat()
    
    def generate_json_report(self, output_path: pathlib.Path) -> Dict:
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        report = {
            'timestamp': self.timestamp,
            'version': '1.0',
            'summary': self._generate_summary(),
            'static_analysis': self.analysis,
            'bottleneck_detection': self.bottlenecks
        }
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            return report
        except Exception as e:
            print(f"âŒ ç”ŸæˆJSONæŠ¥å‘Šå¤±è´¥: {e}", file=sys.stderr)
            return {}
    
    def generate_markdown_report(self, output_path: pathlib.Path) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Šï¼ˆäººç±»å¯è¯»ï¼‰"""
        summary = self._generate_summary()
        
        md = f"# æ•°æ®æµåˆ†ææŠ¥å‘Š\n\n"
        md += f"> **ç”Ÿæˆæ—¶é—´**: {self.timestamp}\n\n"
        md += "---\n\n"
        
        # æ‘˜è¦
        md += "## ğŸ“Š åˆ†ææ‘˜è¦\n\n"
        md += f"- **Criticalé—®é¢˜**: {summary['critical_count']} ä¸ª\n"
        md += f"- **Highé—®é¢˜**: {summary['high_count']} ä¸ª\n"
        md += f"- **Mediumé—®é¢˜**: {summary['medium_count']} ä¸ª\n"
        md += f"- **Lowå»ºè®®**: {summary['low_count']} ä¸ª\n"
        md += f"- **æ€»è®¡**: {summary['total_issues']} ä¸ª\n\n"
        md += "---\n\n"
        
        # Criticalé—®é¢˜
        if summary['critical_count'] > 0:
            md += "## ğŸ”´ Criticalé—®é¢˜ï¼ˆéœ€ç«‹å³å¤„ç†ï¼‰\n\n"
            md += self._format_issues_markdown(self._get_issues_by_severity('critical'))
            md += "\n---\n\n"
        
        # Highé—®é¢˜
        if summary['high_count'] > 0:
            md += "## ğŸŸ  Highé—®é¢˜ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰\n\n"
            md += self._format_issues_markdown(self._get_issues_by_severity('high'))
            md += "\n---\n\n"
        
        # Mediumé—®é¢˜
        if summary['medium_count'] > 0:
            md += "## ğŸŸ¡ Mediumé—®é¢˜ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰\n\n"
            md += self._format_issues_markdown(self._get_issues_by_severity('medium'))
            md += "\n---\n\n"
        
        # Lowå»ºè®®
        if summary['low_count'] > 0:
            md += "## ğŸŸ¢ Lowå»ºè®®ï¼ˆä¼˜åŒ–å»ºè®®ï¼‰\n\n"
            md += self._format_issues_markdown(self._get_issues_by_severity('low'))
            md += "\n---\n\n"
        
        # ä¼˜åŒ–å»ºè®®Top 5
        md += "## ğŸ¯ ä¼˜åŒ–å»ºè®®Top 5\n\n"
        md += self._format_top_recommendations()
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(md)
            return md
        except Exception as e:
            print(f"âŒ ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {e}", file=sys.stderr)
            return ""
    
    def _generate_summary(self) -> Dict:
        """ç”Ÿæˆæ‘˜è¦ä¿¡æ¯"""
        all_issues = []
        
        # æ”¶é›†æ‰€æœ‰é—®é¢˜
        for category, issues in self.analysis.items():
            all_issues.extend(issues)
        
        for category, issues in self.bottlenecks.items():
            if category != 'prioritized_issues':
                all_issues.extend(issues)
        
        # æŒ‰ä¸¥é‡æ€§ç»Ÿè®¡
        severity_counts = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        for issue in all_issues:
            severity = issue.get('severity', 'low')
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
        
        return {
            'total_issues': len(all_issues),
            'critical_count': severity_counts['critical'],
            'high_count': severity_counts['high'],
            'medium_count': severity_counts['medium'],
            'low_count': severity_counts['low']
        }
    
    def _get_issues_by_severity(self, severity: str) -> List[Dict]:
        """è·å–æŒ‡å®šä¸¥é‡æ€§çš„é—®é¢˜"""
        issues = []
        
        for category, items in self.analysis.items():
            for issue in items:
                if issue.get('severity') == severity:
                    issues.append(issue)
        
        for category, items in self.bottlenecks.items():
            if category != 'prioritized_issues':
                for issue in items:
                    if issue.get('severity') == severity:
                        issues.append(issue)
        
        return issues
    
    def _format_issues_markdown(self, issues: List[Dict]) -> str:
        """æ ¼å¼åŒ–é—®é¢˜ä¸ºMarkdown"""
        if not issues:
            return "æ— é—®é¢˜\n"
        
        md = ""
        for i, issue in enumerate(issues, 1):
            issue_type = issue.get('type', 'unknown')
            description = issue.get('description', 'N/A')
            md += f"{i}. **{issue_type}**: {description}\n"
        
        return md
    
    def _format_top_recommendations(self) -> str:
        """æ ¼å¼åŒ–Topå»ºè®®"""
        prioritized = self.bottlenecks.get('prioritized_issues', [])
        
        if not prioritized:
            return "æš‚æ— ä¼˜åŒ–å»ºè®®\n"
        
        md = ""
        for i, issue in enumerate(prioritized[:5], 1):
            description = issue.get('description', 'N/A')
            severity = issue.get('severity', 'low')
            md += f"{i}. [{severity.upper()}] {description}\n"
        
        return md


# ============================================================================
# æ›´æ–°mainå‡½æ•°ä»¥æ”¯æŒæ–°åŠŸèƒ½
# ============================================================================

if __name__ == '__main__':
    main()

